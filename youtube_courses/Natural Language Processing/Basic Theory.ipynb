{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\nimport nltk\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\nimport nltk\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 73;\n",
       "                var nbb_unformatted_code = \"paragraph = \\\"In the online layer, the 134555 online ingestion service is the entry point to the streaming architecture as it decouples and manages the flow of information from the data sources to the processing and storage components, by providing reliable, high throughput, low latency capablities. It functions as enterprise level data bus. Data is also saved in a long term raw data store, but is also a pass through layer to the next online streaming service for real time processing\\\"\";\n",
       "                var nbb_formatted_code = \"paragraph = \\\"In the online layer, the 134555 online ingestion service is the entry point to the streaming architecture as it decouples and manages the flow of information from the data sources to the processing and storage components, by providing reliable, high throughput, low latency capablities. It functions as enterprise level data bus. Data is also saved in a long term raw data store, but is also a pass through layer to the next online streaming service for real time processing\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paragraph = \"In the online layer, the 134555 online ingestion service is the entry point to the streaming architecture as it decouples and manages the flow of information from the data sources to the processing and storage components, by providing reliable, high throughput, low latency capablities. It functions as enterprise level data bus. Data is also saved in a long term raw data store, but is also a pass through layer to the next online streaming service for real time processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 36;\n",
       "                var nbb_unformatted_code = \"sentences = nltk.sent_tokenize(paragraph)\";\n",
       "                var nbb_formatted_code = \"sentences = nltk.sent_tokenize(paragraph)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"words = nltk.word_tokenize(paragraph)\";\n",
       "                var nbb_formatted_code = \"words = nltk.word_tokenize(paragraph)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemning And Lemmatization\n",
    "> process of reducing infected words to their word stem. For example history and historical will become histori in contrast in lemmatization it will be history. Lemmatization always has a meaning whereas stemning doesn't necessarily has meaning. e.g histori and history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 37;\n",
       "                var nbb_unformatted_code = \"from nltk.corpus import stopwords\\nimport copy\";\n",
       "                var nbb_formatted_code = \"from nltk.corpus import stopwords\\nimport copy\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 46;\n",
       "                var nbb_unformatted_code = \"# Steming\\ndef stemming(sentences):\\n    import re\\n    from nltk.stem import PorterStemmer\\n\\n    stemmer = PorterStemmer()\\n    corpus = []\\n    for i in range(len(sentences)):\\n        sentence = re.sub(\\\"[^a-zA-z]\\\", \\\" \\\", sentences[i])\\n        sentence = sentence.lower()\\n        sentence = sentence.split()\\n        words = [\\n            stemmer.stem(word)\\n            for word in sentence\\n            if word not in set(stopwords.words(\\\"english\\\"))\\n        ]\\n        sentence = \\\" \\\".join(words)\\n        corpus.append(sentence)\\n    return corpus\";\n",
       "                var nbb_formatted_code = \"# Steming\\ndef stemming(sentences):\\n    import re\\n    from nltk.stem import PorterStemmer\\n\\n    stemmer = PorterStemmer()\\n    corpus = []\\n    for i in range(len(sentences)):\\n        sentence = re.sub(\\\"[^a-zA-z]\\\", \\\" \\\", sentences[i])\\n        sentence = sentence.lower()\\n        sentence = sentence.split()\\n        words = [\\n            stemmer.stem(word)\\n            for word in sentence\\n            if word not in set(stopwords.words(\\\"english\\\"))\\n        ]\\n        sentence = \\\" \\\".join(words)\\n        corpus.append(sentence)\\n    return corpus\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Steming\n",
    "def stemming(sentences):\n",
    "    import re\n",
    "    from nltk.stem import PorterStemmer\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    corpus = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = re.sub(\"[^a-zA-z]\", \" \", sentences[i])\n",
    "        sentence = sentence.lower()\n",
    "        sentence = sentence.split()\n",
    "        words = [\n",
    "            stemmer.stem(word)\n",
    "            for word in sentence\n",
    "            if word not in set(stopwords.words(\"english\"))\n",
    "        ]\n",
    "        sentence = \" \".join(words)\n",
    "        corpus.append(sentence)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 45;\n",
       "                var nbb_unformatted_code = \"# Lemmatization\\ndef lemmatization(sentences):\\n    import re\\n    from nltk.stem import WordNetLemmatizer\\n\\n    lemmatizer = WordNetLemmatizer()\\n    corpus = []\\n    for i in range(len(sentences)):\\n        sentence = re.sub(\\\"[^a-zA-z]\\\", \\\" \\\", sentences[i])\\n        sentence = sentence.lower()\\n        sentence = sentence.split()\\n        words = [\\n            lemmatizer.lemmatize(word)\\n            for word in sentence\\n            if word not in set(stopwords.words(\\\"english\\\"))\\n        ]\\n        sentence = \\\" \\\".join(words)\\n        corpus.append(sentence)\\n    return corpus\";\n",
       "                var nbb_formatted_code = \"# Lemmatization\\ndef lemmatization(sentences):\\n    import re\\n    from nltk.stem import WordNetLemmatizer\\n\\n    lemmatizer = WordNetLemmatizer()\\n    corpus = []\\n    for i in range(len(sentences)):\\n        sentence = re.sub(\\\"[^a-zA-z]\\\", \\\" \\\", sentences[i])\\n        sentence = sentence.lower()\\n        sentence = sentence.split()\\n        words = [\\n            lemmatizer.lemmatize(word)\\n            for word in sentence\\n            if word not in set(stopwords.words(\\\"english\\\"))\\n        ]\\n        sentence = \\\" \\\".join(words)\\n        corpus.append(sentence)\\n    return corpus\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "def lemmatization(sentences):\n",
    "    import re\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    corpus = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = re.sub(\"[^a-zA-z]\", \" \", sentences[i])\n",
    "        sentence = sentence.lower()\n",
    "        sentence = sentence.split()\n",
    "        words = [\n",
    "            lemmatizer.lemmatize(word)\n",
    "            for word in sentence\n",
    "            if word not in set(stopwords.words(\"english\"))\n",
    "        ]\n",
    "        sentence = \" \".join(words)\n",
    "        corpus.append(sentence)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 92;\n",
       "                var nbb_unformatted_code = \"from gensim.models import Word2Vec\";\n",
       "                var nbb_formatted_code = \"from gensim.models import Word2Vec\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 61;\n",
       "                var nbb_unformatted_code = \"# bag of words\\n# Disadvantage is that there is no sementic meaning\\ndef bagOfWords():\\n    from sklearn.feature_extraction.text import CountVectorizer\\n\\n    cv = CountVectorizer()\\n    corpus = lemmatization(sentences)\\n    x = cv.fit_transform(corpus).toarray()\\n    return x\";\n",
       "                var nbb_formatted_code = \"# bag of words\\n# Disadvantage is that there is no sementic meaning\\ndef bagOfWords():\\n    from sklearn.feature_extraction.text import CountVectorizer\\n\\n    cv = CountVectorizer()\\n    corpus = lemmatization(sentences)\\n    x = cv.fit_transform(corpus).toarray()\\n    return x\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bag of words\n",
    "# Disadvantage is that there is no sementic meaning\n",
    "def bagOfWords():\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    corpus = lemmatization(sentences)\n",
    "    x = cv.fit_transform(corpus).toarray()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 62;\n",
       "                var nbb_unformatted_code = \"bagOfWords = bagOfWords()\";\n",
       "                var nbb_formatted_code = \"bagOfWords = bagOfWords()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bagOfWords = bagOfWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __TF-IDF__ -: Term Frequency and Inverse Document Frequency => __TF*IDF__\n",
    "\n",
    "> __TF__ = number of rep of word in sentence / number of words\n",
    "\n",
    "> __IDF__ = log(number of sentences / number of sentences containing word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 67;\n",
       "                var nbb_unformatted_code = \"# TF-IDF\\ndef tfidf():\\n    from sklearn.feature_extraction.text import TfidfVectorizer\\n    corpus = lemmatization(sentences)\\n    cv = TfidfVectorizer()\\n    X = cv.fit_transform(corpus).toarray()\\n    return X\";\n",
       "                var nbb_formatted_code = \"# TF-IDF\\ndef tfidf():\\n    from sklearn.feature_extraction.text import TfidfVectorizer\\n\\n    corpus = lemmatization(sentences)\\n    cv = TfidfVectorizer()\\n    X = cv.fit_transform(corpus).toarray()\\n    return X\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "def tfidf():\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    corpus = lemmatization(sentences)\n",
    "    cv = TfidfVectorizer()\n",
    "    X = cv.fit_transform(corpus).toarray()\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 83;\n",
       "                var nbb_unformatted_code = \"# Word2Vec\\n# Text Pre-processing\\nparagraph = re.sub(r\\\"\\\\[[0-9]*\\\\]\\\", \\\" \\\", paragraph)\\nparagraph = re.sub(r\\\"\\\\s+\\\", \\\" \\\", paragraph)\\nparagraph = paragraph.lower()\\nparagraph = re.sub(r\\\"\\\\d\\\", \\\" \\\", paragraph)\\nparagraph = re.sub(r\\\"\\\\s+\\\", \\\" \\\", paragraph)\";\n",
       "                var nbb_formatted_code = \"# Word2Vec\\n# Text Pre-processing\\nparagraph = re.sub(r\\\"\\\\[[0-9]*\\\\]\\\", \\\" \\\", paragraph)\\nparagraph = re.sub(r\\\"\\\\s+\\\", \\\" \\\", paragraph)\\nparagraph = paragraph.lower()\\nparagraph = re.sub(r\\\"\\\\d\\\", \\\" \\\", paragraph)\\nparagraph = re.sub(r\\\"\\\\s+\\\", \\\" \\\", paragraph)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Word2Vec\n",
    "# Text Pre-processing\n",
    "paragraph = re.sub(r\"\\[[0-9]*\\]\", \" \", paragraph)\n",
    "paragraph = re.sub(r\"\\s+\", \" \", paragraph)\n",
    "paragraph = paragraph.lower()\n",
    "paragraph = re.sub(r\"\\d\", \" \", paragraph)\n",
    "paragraph = re.sub(r\"\\s+\", \" \", paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 93;\n",
       "                var nbb_unformatted_code = \"model = Word2Vec(sentences, min_count=1)\";\n",
       "                var nbb_formatted_code = \"model = Word2Vec(sentences, min_count=1)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 95;\n",
       "                var nbb_unformatted_code = \"words = model.wv.vocab\";\n",
       "                var nbb_formatted_code = \"words = model.wv.vocab\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 97;\n",
       "                var nbb_unformatted_code = \"vector = model.wv[\\\"online\\\"]\";\n",
       "                var nbb_formatted_code = \"vector = model.wv[\\\"online\\\"]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vector = model.wv[\"online\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 100;\n",
       "                var nbb_unformatted_code = \"similar = model.wv.most_similar(\\\"online\\\")\";\n",
       "                var nbb_formatted_code = \"similar = model.wv.most_similar(\\\"online\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similar = model.wv.most_similar(\"online\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('components', 0.17727796733379364),\n",
       " ('streaming', 0.11093050986528397),\n",
       " ('real', 0.0994991883635521),\n",
       " ('also', 0.0966530293226242),\n",
       " ('providing', 0.09485198557376862),\n",
       " ('processing', 0.09397626668214798),\n",
       " ('entry', 0.09362538158893585),\n",
       " ('.', 0.07042409479618073),\n",
       " ('service', 0.057005371898412704),\n",
       " ('enterprise', 0.05542021989822388)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 101;\n",
       "                var nbb_unformatted_code = \"similar\";\n",
       "                var nbb_formatted_code = \"similar\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
